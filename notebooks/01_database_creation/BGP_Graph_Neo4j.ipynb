{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Install and import necessary packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3ebb82e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3ebb82e",
        "outputId": "81f82bb5-d7d6-468b-f24d-032fb273de39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting mrtparse\n",
            "  Downloading mrtparse-2.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Installing collected packages: mrtparse\n",
            "Successfully installed mrtparse-2.2.0\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (3.2)\n",
            "Collecting neo4j\n",
            "  Downloading neo4j-5.14.0.tar.gz (192 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m192.4/192.4 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from neo4j) (2023.3.post1)\n",
            "Building wheels for collected packages: neo4j\n",
            "  Building wheel for neo4j (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for neo4j: filename=neo4j-5.14.0-py3-none-any.whl size=265476 sha256=a4256a1ce57f9da6d608d3414f56d59cc686221dfb8423ca376a609545955573\n",
            "  Stored in directory: /root/.cache/pip/wheels/53/e0/d7/603097e3fed62f821523433801c09e04cd7a7610c7565bd5a3\n",
            "Successfully built neo4j\n",
            "Installing collected packages: neo4j\n",
            "Successfully installed neo4j-5.14.0\n"
          ]
        }
      ],
      "source": [
        "# Install packages\n",
        "!pip install mrtparse\n",
        "!pip install networkx\n",
        "!pip install neo4j"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "6c49a46b",
      "metadata": {
        "id": "6c49a46b"
      },
      "outputs": [],
      "source": [
        "import mrtparse\n",
        "import networkx as nx\n",
        "import os\n",
        "import requests\n",
        "import gzip\n",
        "import shutil\n",
        "from neo4j import GraphDatabase"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Create the Bviews Database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "ed869c8f",
      "metadata": {
        "id": "ed869c8f"
      },
      "outputs": [],
      "source": [
        "# Define a function to create the graph in Neo4j\n",
        "def create_graph(tx, from_node_id, to_node_id, node_name):\n",
        "  node_label = f\"Node_{node_name}\"\n",
        "  query = (\n",
        "      f\"MERGE (fromNode:{node_label} {{id: $from_node_id}}) \"\n",
        "      f\"MERGE (toNode:{node_label} {{id: $to_node_id}}) \"\n",
        "      f\"MERGE (fromNode)-[:CONNECTS_{node_name}]->(toNode)\"\n",
        "  )\n",
        "  print(query)\n",
        "  tx.run(query, from_node_id=from_node_id, to_node_id=to_node_id,node_name=node_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "ec02d8c3",
      "metadata": {
        "id": "ec02d8c3"
      },
      "outputs": [],
      "source": [
        "# Connect to the Neo4j database\n",
        "uri = \"neo4j+s://28e5b287.databases.neo4j.io\"  # Change to match your Neo4j server settings\n",
        "username = \"neo4j\"      # Change to your Neo4j username\n",
        "password = \"z9J3DPCCxGYELn99XDdFbFIBnWOwR5fdn4MiG_Nvdck\"      # Change to your Neo4j password\n",
        "\n",
        "# Create a Neo4j driver\n",
        "driver = GraphDatabase.driver(uri, auth=(username, password))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "31798fa7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31798fa7",
        "outputId": "587ddaef-5508-40c5-9fa3-b1b27231f3fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "https://data.ris.ripe.net/rrc00/2017.09/bview.20170901.0800.gz\n"
          ]
        }
      ],
      "source": [
        "server_url = \"https://data.ris.ripe.net\"\n",
        "rrc_name = \"rrc00\"\n",
        "year = \"2017\"\n",
        "month = \"09\"\n",
        "date = \"01\"\n",
        "time = \"0800\"\n",
        "\n",
        "node_name = \"{}{}{}{}\".format(year,month,date,time)\n",
        "zip_file_name = \"bview.{}{}{}.{}.gz\".format(year,month,date,time)\n",
        "filename = \"bview.{}{}{}.{}\".format(year,month,date,time)\n",
        "\n",
        "remote_url = server_url + \"/\" + rrc_name + \"/\" + year + \".\" + month + \"/\" + zip_file_name\n",
        "print(remote_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "d9eae18e",
      "metadata": {
        "id": "d9eae18e"
      },
      "outputs": [],
      "source": [
        "r = requests.get(remote_url, allow_redirects=True)\n",
        "\n",
        "if(r.status_code==200):\n",
        "\n",
        "    ## Removing the zip file if it exists\n",
        "    if os.path.isfile(zip_file_name):\n",
        "        os.remove(zip_file_name)\n",
        "\n",
        "    ## Removing content file if it exists\n",
        "    if os.path.isfile(filename):\n",
        "        os.remove(filename)\n",
        "\n",
        "    ## Obtaining the zip file\n",
        "    with open(zip_file_name, 'wb') as f:\n",
        "        f.write(r.content)\n",
        "\n",
        "    ## Obtaining the content file\n",
        "    with gzip.open(zip_file_name, 'rb') as f_in:\n",
        "        with open(filename, 'wb') as f_out:\n",
        "            shutil.copyfileobj(f_in, f_out)\n",
        "\n",
        "    ## Removing the zip file after extraction\n",
        "    if os.path.isfile(zip_file_name):\n",
        "        os.remove(zip_file_name)\n",
        "else:\n",
        "    print('File does not exist')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "72407e00",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72407e00",
        "outputId": "9c7468bd-7247-42cf-a01a-d2d1fde85c7e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-13-3d5ee3854cc2>:23: DeprecationWarning: write_transaction has been renamed to execute_write\n",
            "  session.write_transaction(create_graph, curr_list[k+1], curr_list[k])\n"
          ]
        }
      ],
      "source": [
        "check_list = []\n",
        "check_json = []\n",
        "\n",
        "with driver.session() as session:\n",
        "    i = 0\n",
        "    for entry in Reader(filename):\n",
        "\n",
        "\n",
        "        curr_json = entry.data\n",
        "        #check_json.append(curr_json)\n",
        "\n",
        "        if list(curr_json['subtype'].values())[0]== \"RIB_IPV4_UNICAST\":\n",
        "\n",
        "            if 'rib_entries' in curr_json.keys():\n",
        "\n",
        "                entry_count = curr_json.get('entry_count',0)\n",
        "                for j in range(entry_count):\n",
        "                    curr_list = curr_json['rib_entries'][j]['path_attributes'][1]['value'][0]['value']\n",
        "                    #check_list.append(curr_list)\n",
        "\n",
        "                    N = len(curr_list)\n",
        "                    for k in range(N-1):\n",
        "                      if(curr_list[k+1]!=curr_list[k]): ## Avoiding same node cycles\n",
        "                        session.write_transaction(create_graph, curr_list[k+1], curr_list[k], node_name)\n",
        "            else:\n",
        "                curr_list = curr_json['path_attributes'][1]['value'][0]['value']\n",
        "                #check_list.append(curr_list)\n",
        "\n",
        "                N = len(curr_list)\n",
        "                for k in range(N-1):\n",
        "                  if(curr_list[k+1]!=curr_list[k]):\n",
        "                    session.write_transaction(create_graph, curr_list[k+1], curr_list[k], node_name)\n",
        "\n",
        "\n",
        "            i += 1\n",
        "\n",
        "        if(i==1):\n",
        "            break\n",
        "\n",
        "## Removing content file after reading\n",
        "if os.path.isfile(filename):\n",
        "    os.remove(filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Add Attributes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parse ASN report from url\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import re\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "FZ-hsijhN5cU",
      "metadata": {
        "id": "FZ-hsijhN5cU"
      },
      "outputs": [],
      "source": [
        "def fetch_all_ids(driver):\n",
        "    \"\"\"\n",
        "    Fetch all node IDs from a Neo4j database.\n",
        "\n",
        "    This function queries the database to retrieve all IDs of the nodes\n",
        "    and prepends 'AS' to each ID before returning them in a list.\n",
        "\n",
        "    Args:\n",
        "    driver: A Neo4j driver instance used to connect to the database.\n",
        "\n",
        "    Returns:\n",
        "    A list of strings, each representing a node ID with 'AS' prepended.\n",
        "    \"\"\"\n",
        "    with driver.session() as session:\n",
        "        result = session.run(\"MATCH (n) RETURN n.id AS id\")\n",
        "        return [\"AS\" + record[\"id\"] for record in result]\n",
        "\n",
        "# Fetch and print all IDs\n",
        "try:\n",
        "    ids = fetch_all_ids(driver)\n",
        "    ids = list(set(ids))\n",
        "except:\n",
        "    print('Cannot fetch the ids from graphdatabase')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GWcAXmyrL4_b",
      "metadata": {
        "id": "GWcAXmyrL4_b"
      },
      "outputs": [],
      "source": [
        "def clean_as_number(as_number):\n",
        "    return re.sub(r'\\D', '', as_number) if as_number else None\n",
        "\n",
        "# URL from which to scrape content\n",
        "url = 'https://bgp.potaroo.net/cidr/autnums.html'\n",
        "\n",
        "# Send a GET request to the URL\n",
        "response = requests.get(url)\n",
        "\n",
        "# Check if the request was successful\n",
        "if response.status_code == 200:\n",
        "    # Parse the HTML content\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Find the <pre> tag\n",
        "    pre_tag = soup.find('pre')\n",
        "    if pre_tag:\n",
        "        # Find all <a> tags within the <pre> tag\n",
        "        a_tags = pre_tag.find_all('a')\n",
        "\n",
        "        # List to store each row of data\n",
        "        data = []\n",
        "\n",
        "        # Extract the href attribute and text from each <a> tag\n",
        "        for a_tag in a_tags:\n",
        "            href = a_tag.get('href')\n",
        "            text = a_tag.get_text(strip=True)\n",
        "            sibling_text = a_tag.next_sibling\n",
        "\n",
        "            # Append as a tuple to the data list\n",
        "            data.append((href, text, sibling_text))\n",
        "\n",
        "        # Create a DataFrame\n",
        "        df_url = pd.DataFrame(data, columns=['Link', 'Text', 'Sibling Text'])[1:]\n",
        "\n",
        "        # Optionally, save the DataFrame to a file, e.g., CSV\n",
        "        # df_url.to_csv('output.csv', index=False)\n",
        "\n",
        "    else:\n",
        "        print(\"<pre> tag not found in the HTML.\")\n",
        "\n",
        "else:\n",
        "    print(\"Failed to retrieve the webpage\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "base_url = 'https://bgp.potaroo.net'\n",
        "# Check the raw parsing data\n",
        "df_filter = df_url[df_url['Text'].isin(ids)]\n",
        "\n",
        "dic = {}\n",
        "for url in df_filter['Link']:\n",
        "    cur_url = base_url + url\n",
        "\n",
        "    response = requests.get(cur_url)\n",
        "\n",
        "    # Check if the request was successful\n",
        "    if response.status_code == 200:\n",
        "    # Parse the HTML content\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        ul_tags = soup.find_all('ul')\n",
        "        text = \"\"\n",
        "        for ul in ul_tags:\n",
        "          text += ul.get_text()\n",
        "    # Regular expressions to extract the data\n",
        "    as_number_pattern = r\"ASNumber:\\s+(\\d+)|aut-num:\\s+AS(\\d+)\"\n",
        "    as_name_pattern = r\"ASName:\\s+(.+)|as-name:\\s+(.+)\"\n",
        "    org_name_pattern = r\"OrgName:\\s+(.+)|org-name:\\s+(.+)\"\n",
        "    country_pattern = r\"(?i)Country:\\s+(.+)\"\n",
        "    city_pattern = r\"City:\\s+(.+)\"\n",
        "    state_pattern = r\"StateProv:\\s+(.+)|State:\\s+(.+)\"\n",
        "    adjacent_asn_pattern = r\"Upstream Adjacent AS list\\n(.+)\"\n",
        "    upstream_pattern = r\"Upstream:\\s+(\\d+)\"\n",
        "    downstream_pattern = r\"Downstream:\\s+(\\d+)\"\n",
        "    rank_pattern = r\"Rank\\s+AS\\s+Type\\s+Originate Addr Space\\s+\\(pfx\\)\\s+Transit Addr space\\s+\\(pfx\\)\\s+Description\\n(\\d+)\"\n",
        "    if re.search(as_number_pattern, text):\n",
        "        # Extracting data using regex with checks\n",
        "        as_number = re.search(as_number_pattern, text).group()\n",
        "        as_number = clean_as_number(as_number)\n",
        "        as_name = re.search(as_name_pattern, text).group(1) if re.search(as_name_pattern, text).group(1) else re.search(as_name_pattern, text).group(2)\n",
        "        org_name_search = re.search(org_name_pattern, text)\n",
        "        if org_name_search:\n",
        "            org_name = org_name_search.group(1) if org_name_search.group(1) else org_name_search.group(2)\n",
        "        else:\n",
        "            org_name = None\n",
        "        #org_name = re.search(org_name_pattern, text).group(1) if re.search(org_name_pattern, text).group(1) else re.search(org_name_pattern, text).group(2)\n",
        "        country = re.search(country_pattern, text).group(1) if re.search(country_pattern, text) else None\n",
        "        city = re.search(city_pattern, text).group(1) if re.search(city_pattern, text) else None\n",
        "        state = re.search(state_pattern, text).group(1) if re.search(state_pattern, text) else None\n",
        "        upstream = re.search(upstream_pattern, text).group(1) if re.search(upstream_pattern, text) else None\n",
        "        downstream = re.search(downstream_pattern, text).group(1) if re.search(downstream_pattern, text) else None\n",
        "        rank = re.search(rank_pattern, text).group(1) if re.search(rank_pattern, text) else None\n",
        "\n",
        "        dic[as_number] = [as_name,org_name, country, city,state,upstream,downstream, rank]\n",
        "\n",
        "data_tuples = [(key, *values) for key, values in dic.items()]\n",
        "df = pd.DataFrame(data_tuples,columns=[\"ASN\",\"as_name\",\"org_name\", \"country\", \"city\",\"state\", \"upstream\",\"downstream\", \"rank\"] )\n",
        "# Save it into csv file\n",
        "df.to_csv('data/attri.csv', index = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Update the Neo4j Aura database\n",
        "def update_node(driver, attributes):\n",
        "    with driver.session() as session:\n",
        "        session.run(\n",
        "            \"\"\"\n",
        "            MATCH (n) WHERE n.id = $ASN\n",
        "            SET n.as_name = $as_name, n.org_name = $org_name, n.country = $country,\n",
        "                n.city = $city, n.state = $state,n.upstream = $upstream,\n",
        "                n.downstream = $downstream, n.rank = $rank\n",
        "            \"\"\",\n",
        "            **attributes\n",
        "        )\n",
        "\n",
        "try:\n",
        "    for _, row in df.iterrows():\n",
        "        update_node(driver, row.to_dict())\n",
        "finally:\n",
        "    driver.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Optional(Add BGP update Message to another GraphDatabase)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def download_gz_file(url, filename):\n",
        "    \"\"\"\n",
        "    Download a .gz file from the given URL and save it as filename.\n",
        "    \"\"\"\n",
        "    response = requests.get(url, stream=True)\n",
        "    # Check if the response was successful\n",
        "    if response.status_code == 200:\n",
        "        # Save the file to disk\n",
        "        with open(filename, 'wb') as f:\n",
        "            f.write(response.content)\n",
        "        print(f\"Downloaded: {filename}\")\n",
        "    else:\n",
        "        # Print an error message if the download failed\n",
        "        print(f\"Failed to download. Status code: {response.status_code}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import datetime\n",
        "\n",
        "now = datetime.datetime.now()\n",
        "today = datetime.date.today()\n",
        "month = now.month\n",
        "year = now.year\n",
        "hour = now.hour\n",
        "\n",
        "base_url = \"https://data.ris.ripe.net/rrc11/\"\n",
        "url = base_url + str(year) + \".\" + str(month)\n",
        "part1 = now.strftime(\"%Y%m%d\")\n",
        "part2 = now.strftime(\"%H00\")\n",
        "url += \"/updates.\" + part1 + \".\" + part2 + \".gz\"\n",
        "\n",
        "download_gz_file(url, f'{today}.gz')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# unzip the file and saved the data in a list\n",
        "\n",
        "with gzip.open(f'{today}.gz', 'rb') as f_in:\n",
        "  with open(f'{today}.txt', 'wb') as f_out:\n",
        "    shutil.copyfileobj(f_in, f_out)\n",
        "\n",
        "message_list = []\n",
        "# Reader is from mrtparse package, it works as unstructure the mrt format. \n",
        "for entry in Reader(f'{today}.txt'):\n",
        "    message_list.append(entry.data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Parse the data from the each message in List"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There are four types of patterns: keepalive, update, open, notification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to process each OrderedDict\n",
        "def process_ordered_dict(tx, bgp_data,message_id):\n",
        "    # Convert all keys to strings and check for 'bgp_message'\n",
        "    bgp_data = {str(key): value for key, value in bgp_data.items()}\n",
        "    if 'bgp_message' not in bgp_data:\n",
        "        # Skip this row if 'bgp_message' is not present\n",
        "        return\n",
        "\n",
        "    local_as = bgp_data.get('local_as')\n",
        "    peer_as = bgp_data.get('peer_as')\n",
        "    timestamp = list(bgp_data['timestamp'].values())[0]\n",
        "\n",
        "    bgp_message = bgp_data['bgp_message']\n",
        "    message_type = list(bgp_message['type'].values())[0]\n",
        "    message_length = bgp_message.get('length')\n",
        "\n",
        "    # Create or merge Local AS and Peer AS nodes\n",
        "    tx.run(\"MERGE (localAS:AS {id: $local_as})\", local_as=local_as)\n",
        "    tx.run(\"MERGE (peerAS:AS {id: $peer_as})\", peer_as=peer_as)\n",
        "\n",
        "    if message_type == 'UPDATE':\n",
        "      withdrawn_routes_length = len(bgp_message.get('withdrawn_routes', []))\n",
        "      nlri_length = len(bgp_message.get('nlri', []))\n",
        "      is_withdrawal = withdrawn_routes_length > 0\n",
        "      is_announcement = nlri_length > 0\n",
        "\n",
        "      announcements_history = {}\n",
        "      # Create relationship based on message type\n",
        "      if is_withdrawal:\n",
        "        withdrawn_list = [x['prefix'] for x in bgp_message['withdrawn_routes']]\n",
        "        tx.run(\"\"\"\n",
        "              MATCH (localAS:AS {id: $local_as}), (peerAS:AS {id: $peer_as})\n",
        "              CREATE (localAS)-[r:WITHDRAWS_ROUTE_TO {m_id: $message_id}]->(peerAS)\n",
        "              SET r.timestamp = $timestamp, r.message_type = $message_type, r.message_length = $message_length, r.routes_length = $withdrawn_routes_length, r.withdrawn_list = $withdrawn_list\n",
        "              \"\"\", local_as=local_as, peer_as=peer_as,timestamp=timestamp,\n",
        "               message_type=message_type, message_length=message_length,\n",
        "               withdrawn_routes_length=withdrawn_routes_length,message_id=message_id, withdrawn_list=withdrawn_list)\n",
        "      if is_announcement:\n",
        "        path_seq = list(bgp_message['path_attributes'][1]['value'][0]['value'])\n",
        "        prefix = bgp_message['nlri'][0]['prefix']\n",
        "        announcement_key = (local_as, peer_as, prefix)\n",
        "        attributes = [str(x) for x in list(bgp_message.get('path_attributes',[]))]\n",
        "        # Check for new, duplicate, or implicit withdrawal\n",
        "        if announcement_key not in announcements_history:\n",
        "            category = 'new_announcement'\n",
        "        elif announcements_history[announcement_key] == attributes:\n",
        "            category = 'duplicate_announcement'\n",
        "        else:\n",
        "            category = 'implicit_withdrawal'\n",
        "        tx.run(\"\"\"\n",
        "              MATCH (localAS:AS {id: $local_as}), (peerAS:AS {id: $peer_as})\n",
        "              CREATE (localAS)-[r:ANNOUNCES_ROUTE_TO {m_id: $message_id, category:$category}]->(peerAS)\n",
        "              SET r.timestamp = $timestamp, r.message_type = $message_type, r.message_length = $message_length,\n",
        "              r.routes_length = $nlri_length, r.path_seq = $path_seq\n",
        "              \"\"\", local_as=local_as, peer_as=peer_as, timestamp=timestamp,\n",
        "               message_type=message_type, message_length=message_length,\n",
        "               nlri_length=nlri_length,message_id=message_id, path_seq=path_seq, category=category)\n",
        "    elif message_type == 'OPEN':\n",
        "      # Create relationship based on message type\n",
        "      open_version = bgp_message.get('version')\n",
        "      open_local_as = bgp_message.get('local_as')\n",
        "      open_holdtime = bgp_message.get('holdtime')\n",
        "      open_bgp_id = bgp_message.get('bgp_id')\n",
        "      # Handle OPEN message\n",
        "      tx.run(\"\"\"\n",
        "            MATCH (localAS:AS {id: $local_as}), (peerAS:AS {id: $peer_as})\n",
        "            CREATE (localAS)-[r:OPENS_CONNECTION_TO {m_id: $message_id}]->(peerAS)\n",
        "            SET r.timestamp = $timestamp, r.message_length = $message_length,\n",
        "                r.version = $version, r.open_local_as = $open_local_as,\n",
        "                r.holdtime = $holdtime, r.bgp_id = $bgp_id\n",
        "            \"\"\", local_as=local_as, peer_as=peer_as, timestamp=timestamp,\n",
        "                message_length=message_length, version=open_version,\n",
        "                open_local_as=open_local_as, holdtime=open_holdtime, bgp_id=open_bgp_id,message_id=message_id)\n",
        "    elif message_type == 'NOTIFICATION':\n",
        "      # Handle NOTIFICATION message\n",
        "      error_code = list(bgp_message['error_code'].values())[0]\n",
        "      error_subcode = list(bgp_message.get('error_subcode'))[0]\n",
        "      # Handle NOTIFICATION message\n",
        "      tx.run(\"\"\"\n",
        "            MATCH (localAS:AS {id: $local_as}), (peerAS:AS {id: $peer_as})\n",
        "            CREATE (localAS)-[r:SENDS_NOTIFICATION_TO {m_id: $message_id}]->(peerAS)\n",
        "            SET r.timestamp = $timestamp, r.message_length = $message_length,\n",
        "                r.error_code = $error_code, r.error_subcode = $error_subcode\n",
        "            \"\"\", local_as=local_as, peer_as=peer_as, timestamp=timestamp,\n",
        "                message_length=message_length, error_code=error_code,\n",
        "                error_subcode=error_subcode,message_id=message_id)\n",
        "    elif message_type == 'KEEPALIVE':\n",
        "      # Handle KEEPALIVE message\n",
        "      tx.run(\"\"\"\n",
        "            MATCH (localAS:AS {id: $local_as}), (peerAS:AS {id: $peer_as})\n",
        "            CREATE (localAS)-[r:SENDS_KEEPALIVE_TO {m_id: $message_id}]->(peerAS)\n",
        "            SET r.timestamp = $timestamp, r.message_length = $message_length\n",
        "            \"\"\", local_as=local_as, peer_as=peer_as, timestamp=timestamp, message_length=message_length,message_id=message_id)\n",
        "# Connect to Neo4j\n",
        "driver = GraphDatabase.driver(\"neo4j+s://896675cf.databases.neo4j.io\", auth=(\"neo4j\", \"vDR6onHDPNATBPsRKm4Crvk7EDSii3AuNTKxX_DZgdM\"))\n",
        "\n",
        "# Process each OrderedDict\n",
        "with driver.session() as session:\n",
        "  for i, bgp_dict in enumerate(tqdm(message_list, desc=\"Processing BGP Messages\")):\n",
        "        session.execute_write(process_ordered_dict, bgp_dict, message_id=i)\n",
        "driver.close()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 ('client_app': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "12351c5ee4dcd67e501ce7129fce3577c6627ea103323f32e3fe824d00dd2655"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
