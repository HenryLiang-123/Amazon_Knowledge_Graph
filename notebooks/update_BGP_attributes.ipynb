{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PzbzWn8RQNs-",
        "outputId": "e2ba0e70-5735-4987-eea5-d2764220ca62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting neo4j\n",
            "  Downloading neo4j-5.14.1.tar.gz (192 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/192.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m143.4/192.8 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m192.8/192.8 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from neo4j) (2023.3.post1)\n",
            "Building wheels for collected packages: neo4j\n",
            "  Building wheel for neo4j (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for neo4j: filename=neo4j-5.14.1-py3-none-any.whl size=267559 sha256=2792a78c48f73cf4648e3cc9b90091373f5cac7bd176894adaf7908eef12717b\n",
            "  Stored in directory: /root/.cache/pip/wheels/13/01/ff/de2142e172dafcd475f118499db2140c66562e42fd500c460a\n",
            "Successfully built neo4j\n",
            "Installing collected packages: neo4j\n",
            "Successfully installed neo4j-5.14.1\n"
          ]
        }
      ],
      "source": [
        "!pip install neo4j"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xdeUbQMPPGW",
        "outputId": "2313c0a0-7765-4b0e-eb9f-ac732f4be054"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['AS42708', 'AS50304', 'AS42708', 'AS50304', 'AS3356', 'AS45896', 'AS3356', 'AS45896', 'AS6830', 'AS8758', 'AS6830', 'AS8758', 'AS6939', 'AS6939', 'AS4826', 'AS4826', 'AS38803', 'AS38803', 'AS56203', 'AS56203', 'AS2497', 'AS4777', 'AS2497', 'AS4777', 'AS4844', 'AS37989', 'AS4844', 'AS37989', 'AS4608', 'AS4608', 'AS57821', 'AS57821', 'AS3549', 'AS3549', 'AS1299', 'AS1299', 'AS29608', 'AS29608', 'AS22652', 'AS22652', 'AS57381', 'AS57381', 'AS50300', 'AS50300', 'AS3257', 'AS3257', 'AS7018', 'AS7018', 'AS1836', 'AS1836', 'AS3333', 'AS3333', 'AS2516', 'AS2516', 'AS4637', 'AS4637', 'AS1221', 'AS1221', 'AS10026', 'AS10026', 'AS2519', 'AS2519', 'AS7670', 'AS7670', 'AS18144', 'AS18144', 'AS209', 'AS209', 'AS1273', 'AS1273', 'AS38040', 'AS38040', 'AS9737', 'AS9737', 'AS6762', 'AS6762', 'AS12586', 'AS12586', 'AS2914', 'AS2914', 'AS8220', 'AS8220', 'AS4713', 'AS4713', 'AS174', 'AS174', 'AS1103', 'AS1103', 'AS4651', 'AS4651', 'AS23969', 'AS23969', 'AS4766', 'AS4766', 'AS31025', 'AS31025', 'AS1136', 'AS1136', 'AS6453', 'AS6453', 'AS1', 'AS2']\n"
          ]
        }
      ],
      "source": [
        "# fetch all required ASN from the cloud\n",
        "# to fetch all the attributes of each AS online takes quite a lot of time\n",
        "\n",
        "from neo4j import GraphDatabase\n",
        "\n",
        "# Neo4j Aura DB credentials\n",
        "uri = \"neo4j+s://28e5b287.databases.neo4j.io\"\n",
        "username = \"neo4j\"\n",
        "password = \"z9J3DPCCxGYELn99XDdFbFIBnWOwR5fdn4MiG_Nvdck\"\n",
        "\n",
        "# Function to fetch all IDs\n",
        "def fetch_all_ids(driver):\n",
        "    with driver.session() as session:\n",
        "        result = session.run(\"MATCH (n) RETURN n.id AS id\")\n",
        "        return [\"AS\" + record[\"id\"] for record in result]\n",
        "\n",
        "# Connect to Neo4j Aura DB\n",
        "driver = GraphDatabase.driver(uri, auth=(username, password))\n",
        "\n",
        "# Fetch and print all IDs\n",
        "try:\n",
        "    ids = fetch_all_ids(driver)\n",
        "finally:\n",
        "    driver.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7Z_Zo0ztwngC"
      },
      "outputs": [],
      "source": [
        "# remove the duplicaed ids. There are 2 labels in the graph datbase, which contain duplicated ids\n",
        "ids = list(set(ids))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "MmwnwlQuTM78"
      },
      "outputs": [],
      "source": [
        "# Parse ASN report from url\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# URL from which to scrape content\n",
        "url = 'https://bgp.potaroo.net/cidr/autnums.html'\n",
        "\n",
        "# Send a GET request to the URL\n",
        "response = requests.get(url)\n",
        "\n",
        "# Check if the request was successful\n",
        "if response.status_code == 200:\n",
        "    # Parse the HTML content\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Find the <pre> tag\n",
        "    pre_tag = soup.find('pre')\n",
        "    if pre_tag:\n",
        "        # Find all <a> tags within the <pre> tag\n",
        "        a_tags = pre_tag.find_all('a')\n",
        "\n",
        "        # List to store each row of data\n",
        "        data = []\n",
        "\n",
        "        # Extract the href attribute and text from each <a> tag\n",
        "        for a_tag in a_tags:\n",
        "            href = a_tag.get('href')\n",
        "            text = a_tag.get_text(strip=True)\n",
        "            sibling_text = a_tag.next_sibling\n",
        "\n",
        "            # Append as a tuple to the data list\n",
        "            data.append((href, text, sibling_text))\n",
        "\n",
        "        # Create a DataFrame\n",
        "        df_url = pd.DataFrame(data, columns=['Link', 'Text', 'Sibling Text'])[1:]\n",
        "\n",
        "        # Optionally, save the DataFrame to a file, e.g., CSV\n",
        "        # df.to_csv('output.csv', index=False)\n",
        "\n",
        "    else:\n",
        "        print(\"<pre> tag not found in the HTML.\")\n",
        "\n",
        "else:\n",
        "    print(\"Failed to retrieve the webpage\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "cC-Gu0vxTWG3"
      },
      "outputs": [],
      "source": [
        "# Check the raw parsing data\n",
        "df_filter = df_url[df_url['Text'].isin(ids)]\n",
        "df_filter.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "0yIg5vEXWmgi"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "def clean_as_number(as_number):\n",
        "    return re.sub(r'\\D', '', as_number) if as_number else None\n",
        "\n",
        "base_url = 'https://bgp.potaroo.net'\n",
        "\n",
        "dic = {}\n",
        "for url in df_filter['Link']:\n",
        "    cur_url = base_url + url\n",
        "\n",
        "    response = requests.get(cur_url)\n",
        "\n",
        "    # Check if the request was successful\n",
        "    if response.status_code == 200:\n",
        "    # Parse the HTML content\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        ul_tags = soup.find_all('ul')\n",
        "        text = \"\"\n",
        "        for ul in ul_tags:\n",
        "          text += ul.get_text()\n",
        "    # Regular expressions to extract the data\n",
        "    as_number_pattern = r\"ASNumber:\\s+(\\d+)|aut-num:\\s+AS(\\d+)\"\n",
        "    as_name_pattern = r\"ASName:\\s+(.+)|as-name:\\s+(.+)\"\n",
        "    org_name_pattern = r\"OrgName:\\s+(.+)|org-name:\\s+(.+)\"\n",
        "    country_pattern = r\"(?i)Country:\\s+(.+)\"\n",
        "    city_pattern = r\"City:\\s+(.+)\"\n",
        "    state_pattern = r\"StateProv:\\s+(.+)|State:\\s+(.+)\"\n",
        "    adjacent_asn_pattern = r\"Upstream Adjacent AS list\\n(.+)\"\n",
        "    upstream_pattern = r\"Upstream:\\s+(\\d+)\"\n",
        "    downstream_pattern = r\"Downstream:\\s+(\\d+)\"\n",
        "    rank_pattern = r\"Rank\\s+AS\\s+Type\\s+Originate Addr Space\\s+\\(pfx\\)\\s+Transit Addr space\\s+\\(pfx\\)\\s+Description\\n(\\d+)\"\n",
        "    if re.search(as_number_pattern, text):\n",
        "        # Extracting data using regex with checks\n",
        "        as_number = re.search(as_number_pattern, text).group()\n",
        "        as_number = clean_as_number(as_number)\n",
        "        as_name = re.search(as_name_pattern, text).group(1) if re.search(as_name_pattern, text).group(1) else re.search(as_name_pattern, text).group(2)\n",
        "        org_name_search = re.search(org_name_pattern, text)\n",
        "        if org_name_search:\n",
        "            org_name = org_name_search.group(1) if org_name_search.group(1) else org_name_search.group(2)\n",
        "        else:\n",
        "            org_name = None\n",
        "        #org_name = re.search(org_name_pattern, text).group(1) if re.search(org_name_pattern, text).group(1) else re.search(org_name_pattern, text).group(2)\n",
        "        country = re.search(country_pattern, text).group(1) if re.search(country_pattern, text) else None\n",
        "        city = re.search(city_pattern, text).group(1) if re.search(city_pattern, text) else None\n",
        "        state = re.search(state_pattern, text).group(1) if re.search(state_pattern, text) else None\n",
        "        upstream = re.search(upstream_pattern, text).group(1) if re.search(upstream_pattern, text) else None\n",
        "        downstream = re.search(downstream_pattern, text).group(1) if re.search(downstream_pattern, text) else None\n",
        "        rank = re.search(rank_pattern, text).group(1) if re.search(rank_pattern, text) else None\n",
        "\n",
        "        dic[as_number] = [as_name,org_name, country, city,state,upstream,downstream, rank]\n",
        "\n",
        "data_tuples = [(key, *values) for key, values in dic.items()]\n",
        "df = pd.DataFrame(data_tuples,columns=[\"ASN\",\"as_name\",\"org_name\", \"country\", \"city\",\"state\", \"upstream\",\"downstream\", \"rank\"] )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8i1Re-xoyUJ",
        "outputId": "95de0532-f0d4-40ac-c8df-2ec706041452"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ASN            0\n",
              "as_name        0\n",
              "org_name      15\n",
              "country        7\n",
              "city          41\n",
              "state         41\n",
              "upstream       3\n",
              "downstream     3\n",
              "rank           3\n",
              "dtype: int64"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Check missing values\n",
        "df.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "w0wS-ZrTYGkB"
      },
      "outputs": [],
      "source": [
        "# Save it into csv file\n",
        "df.to_csv('data/attri.csv', index = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SVtyGvV-YNyg",
        "outputId": "9c888915-1d20-4c8c-9960-358b5535d48a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-31-025aafc83725>:2: DeprecationWarning: Using a driver after it has been closed is deprecated. Future versions of the driver will raise an error.\n",
            "  with driver.session() as session:\n"
          ]
        }
      ],
      "source": [
        "# Update the Neo4j Aura database\n",
        "def update_node(driver, attributes):\n",
        "    with driver.session() as session:\n",
        "        session.run(\n",
        "            \"\"\"\n",
        "            MATCH (n) WHERE n.id = $ASN\n",
        "            SET n.as_name = $as_name, n.org_name = $org_name, n.country = $country,\n",
        "                n.city = $city, n.state = $state,n.upstream = $upstream,\n",
        "                n.downstream = $downstream, n.rank = $rank\n",
        "            \"\"\",\n",
        "            **attributes\n",
        "        )\n",
        "\n",
        "try:\n",
        "    for _, row in df.iterrows():\n",
        "        update_node(driver, row.to_dict())\n",
        "finally:\n",
        "    driver.close()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
